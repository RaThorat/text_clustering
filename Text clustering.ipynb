{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing various modules required for analysis\n",
    "import xlsxwriter \n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import os\n",
    "os.getcwd()\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the excel. The excel shoud have column named 'Tekst'\n",
    "root= tk.Tk()\n",
    "\n",
    "canvas1 = tk.Canvas(root, width = 300, height = 300, bg = 'lightsteelblue')\n",
    "canvas1.pack()\n",
    "#input training document\n",
    "def getExcel ():\n",
    "    global df_train\n",
    "    \n",
    "    import_file_path = filedialog.askopenfilename()\n",
    "    df_train = pd.read_excel (import_file_path)\n",
    "    #print (df_train)\n",
    "    \n",
    "browseButton_Excel = tk.Button(text='Import Excel File', command=getExcel, bg='green', fg='white', font=('helvetica', 12, 'bold'))\n",
    "canvas1.create_window(150, 150, window=browseButton_Excel)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a column named 'Corpus' from the Text\n",
    "df_train[\"Corpus\"]=df_train[\"Tekst\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text tokenizing, lemmatizing, stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the nan values from text\n",
    "df_train = df_train[df_train['Corpus'].notna()]\n",
    "#dropping the punctuation from text\n",
    "import string\n",
    "string.punctuation\n",
    "def remove_punctuation(txt):\n",
    "    txt_nopunct =\"\".join([c for c in txt if c not in string.punctuation])\n",
    "    return txt_nopunct\n",
    "df_train['Corpus_clean']=df_train['Corpus'].apply(lambda x: remove_punctuation(x))\n",
    "#tokenization\n",
    "import re\n",
    "def tokenize(txt):\n",
    "    tokens= re.split('\\W+', txt)\n",
    "    return tokens\n",
    "df_train['Corpus_clean_tokenized']=df_train['Corpus_clean'].apply(lambda x:tokenize(x.lower()))\n",
    "#removing the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "df_train['Corpus_nostop']=df_train['Corpus_clean_tokenized'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "#lemmatization\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "wn=nltk.WordNetLemmatizer()\n",
    "def lemmatization(token_text):\n",
    "    text= [wn.lemmatize(word) for word in token_text]\n",
    "    return text\n",
    "df_train['Corpus_lemmatized']=df_train['Corpus_nostop'].apply(lambda x: lemmatization(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion of columns to lists\n",
    "train_x=df_train['Corpus_lemmatized'].to_list()\n",
    "\n",
    "#Converting vector values into string values\n",
    "train_x = [str (item) for item in train_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vectorizer usingTfidfVectorizer class to fit and transform the document \n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining no of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K means clusering algorithm\n",
    "true_k = 6 #type number of clusters you required\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a list of labels\n",
    "clusters = model.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the centroids and features\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the centroids into which clusters they belongs\n",
    "for i in range(true_k):\n",
    "    print(\"cluster \", i),\n",
    "    for ind in order_centroids[i, :20]:\n",
    "        print(\" \", terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use joblib.dump to pickle the model, once it has converged and to reload the model/reassign the labels as the clusters.\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the below code to save your model \n",
    "#joblib.dump(model,  'doc_cluster.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since I've already run my model I am loading from the pickle\n",
    "model = joblib.load('doc_cluster.pkl')\n",
    "clusters = model.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cluster_kmeans'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cluster_kmeans'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give name to the output excel\n",
    "df_train.to_excel('name to the output excel.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
